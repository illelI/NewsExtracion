{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a60e9126-89e5-45b2-be1a-f402502ffe98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "import random\n",
    "import time\n",
    "from lxml import html\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import spacy\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sentence_transformers import models, SentenceTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "43545dc5-f228-4ff7-b94a-c72c12ec7922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_article(url, xpath, rate_limiting=False):\n",
    "    if rate_limiting:\n",
    "        user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "            \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:92.0) Gecko/20100101 Firefox/92.0\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.64\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; rv:56.0) Gecko/20100101 Firefox/56.0\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Mobile Safari/537.36\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.115 Safari/537.36\"\n",
    " \n",
    "        ]\n",
    "        headers = {\"User-Agent\": random.choice(user_agents)}\n",
    "        req = urllib.request.Request(url, headers=headers)\n",
    "        response = urllib.request.urlopen(req)\n",
    "    else:\n",
    "        response = urllib.request.urlopen(url)\n",
    "    page_content = response.read()\n",
    "    response.close()\n",
    "    tree = html.fromstring(page_content)\n",
    "    text = tree.xpath(xpath)\n",
    "    return \" \".join(text).strip()\n",
    "\n",
    "def get_wp_articles_urls(url):\n",
    "    response = urllib.request.urlopen(url)\n",
    "    page_content = response.read()\n",
    "    response.close()\n",
    "    tree = html.fromstring(page_content)\n",
    "    xpath = \"//div[@class='akWTRfRe a3brEpa-']//a\"\n",
    "    links = tree.xpath(xpath)\n",
    "    counter = 0\n",
    "    urls = []\n",
    "    for link in links:\n",
    "        if counter % 2 == 0:\n",
    "            urls.append(link.get('href'))\n",
    "        counter+=1\n",
    "    return urls\n",
    "            \n",
    "def create_files(urls, xpath, folder_name, count=1, rate_limiting=False):\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    counter = count\n",
    "    for url in urls:\n",
    "        text = get_text_from_article(url, xpath, rate_limiting)\n",
    "        file_name = os.path.join(folder_name,str(counter) + '.txt')\n",
    "        counter+=1\n",
    "        with open(file_name, 'w', encoding='utf-8') as file:\n",
    "            file.write(text)\n",
    "\n",
    "def get_text_from_wp(url, xpath, folder_name):\n",
    "    try:\n",
    "        urls = get_wp_articles_urls(url)\n",
    "        page = 2\n",
    "        while True:\n",
    "            new_url = url + '/' + str(page)\n",
    "            page+=1\n",
    "            for url1 in get_wp_articles_urls(new_url):\n",
    "                urls.append(url1)\n",
    "    except Exception as e:\n",
    "        create_files(urls, xpath, folder_name)\n",
    "\n",
    "def get_bi_articles_urls(url):\n",
    "    response = urllib.request.urlopen(url)\n",
    "    page_content = response.read()\n",
    "    response.close()\n",
    "    tree = html.fromstring(page_content)\n",
    "    xpath = \"//main//div[@class='stream-list']//a\"\n",
    "    links = tree.xpath(xpath)\n",
    "    urls = []\n",
    "    for link in links:\n",
    "        urls.append(link.get(\"href\"))\n",
    "    return urls\n",
    "\n",
    "def get_text_from_bi(url, xpath, folder_name):\n",
    "    old_size = 0\n",
    "    urls = get_bi_articles_urls(url)\n",
    "    new_size = len(urls)\n",
    "    page = 2\n",
    "    while old_size != new_size:\n",
    "        new_url = url + '?page=' + str(page)\n",
    "        page+=1\n",
    "        old_size = new_size\n",
    "        for url1 in get_bi_articles_urls(new_url):\n",
    "            urls.append(url1)\n",
    "        new_size = len(urls)\n",
    "    create_files(urls, xpath, folder_name)\n",
    "\n",
    "def get_kryminalki_urls(url):\n",
    "    user_agents = [\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:89.0) Gecko/20100101 Firefox/89.0\",\n",
    "            \"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:92.0) Gecko/20100101 Firefox/92.0\",\n",
    "            \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36 Edg/91.0.864.64\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; rv:56.0) Gecko/20100101 Firefox/56.0\",\n",
    "            \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\",\n",
    "            \"Mozilla/5.0 (iPhone; CPU iPhone OS 14_6 like Mac OS X) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.77 Mobile Safari/537.36\",\n",
    "            \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.115 Safari/537.36\"\n",
    " \n",
    "        ]\n",
    "    headers = {\"User-Agent\": random.choice(user_agents)}\n",
    "    req = urllib.request.Request(url, headers=headers)\n",
    "    response = urllib.request.urlopen(req)\n",
    "    page_content = response.read()\n",
    "    response.close()\n",
    "    tree = html.fromstring(page_content)\n",
    "    xpath = \"//div[@class='article-item']//a\"\n",
    "    links = tree.xpath(xpath)\n",
    "    urls = []\n",
    "    for link in links:\n",
    "        urls.append('https://www.kryminalki.pl' + link.get(\"href\"))\n",
    "    return urls\n",
    "\n",
    "def get_text_from_kryminalki(url, xpath, folder_name):\n",
    "    urls = get_kryminalki_urls(url)\n",
    "    for i in range(2, 200):\n",
    "        time.sleep(5)\n",
    "        new_url = url + '?p=' + str(i)\n",
    "        for url1 in get_kryminalki_urls(new_url):\n",
    "            urls.append(url1)\n",
    "    create_files(urls, xpath, folder_name, 63, True)\n",
    "\n",
    "\n",
    "\n",
    "xpath_wp = '//article//h1//text() | //article//li//text() | //article//p//text() | //article//h2//text()'\n",
    "xpath_bi = \"//p[@class='article_p']//text()\"\n",
    "xpath_kryminalki = \"//h1[@class='text-break text-title']//text() | //div[@class='my-3 text-break text-lead']//text() | //div[@id='article-content']//text()\"\n",
    "url_przestepstwa = 'https://wiadomosci.wp.pl/tag/przest%C4%99pstwa'\n",
    "url_katastrofa = 'https://wiadomosci.wp.pl/tag/katastrofa'\n",
    "url_biznes = 'https://businessinsider.com.pl/biznes'\n",
    "url_przestepstwa2 = 'https://www.kryminalki.pl/artykuly/45/przestepcy'\n",
    "#get_text_from_wp(url_przestepstwa, xpath_wp, 'przestępstwo')\n",
    "#get_text_from_wp(url_katastrofa, xpath_wp, 'katastrofa')\n",
    "#get_text_from_bi(url_biznes, xpath_bi, 'biznes')\n",
    "get_text_from_kryminalki(url_przestepstwa2, xpath_kryminalki, 'przestępstwo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fc346f5-73ae-4581-8ffc-d90aa80db5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pl_core_news_sm')\n",
    "\n",
    "\n",
    "def split_text_into_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    return [sent.text for sent in doc.sents]\n",
    "\n",
    "def create_files(category, ratio):\n",
    "    train_counter = 1\n",
    "    test_counter = 1\n",
    "    for filename in os.listdir(category):\n",
    "        with open(category + '/' + filename, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            sentences = split_text_into_sentences(text)\n",
    "            for sentence in sentences:\n",
    "                if random.randint(0, 9) > ratio:\n",
    "                    path = 'dataset/train/' + category + '/' + str(train_counter) + '.txt'\n",
    "                    train_counter+=1\n",
    "                else:\n",
    "                    path = 'dataset/test/' + category + '/' + str(test_counter) + '.txt'\n",
    "                    test_counter+=1\n",
    "                with open(path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(sentence)\n",
    "                \n",
    "\n",
    "def divide_into_test_and_trening(test_size=0.2):\n",
    "    ratio = test_size * 10\n",
    "    categories = ['biznes', 'katastrofa', 'przestępstwo']\n",
    "    try:\n",
    "        shutil.rmtree('dataset')\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    for category in categories:\n",
    "        os.makedirs('dataset/test/' + category, exist_ok=True)\n",
    "        os.makedirs('dataset/train/' + category, exist_ok=True)\n",
    "        create_files(category, ratio)\n",
    "\n",
    "divide_into_test_and_trening()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c72462d-2c8a-43d6-a86e-759550658b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data_from_folder(data_dir, label_map=None):\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    if label_map is None:\n",
    "        label_map = {\n",
    "            d: idx\n",
    "            for idx, d in enumerate(\n",
    "                sorted([\n",
    "                    x for x in os.listdir(data_dir)\n",
    "                    if os.path.isdir(os.path.join(data_dir, x))\n",
    "                ])\n",
    "            )\n",
    "        }\n",
    "\n",
    "    for label, label_id in label_map.items():\n",
    "        label_path = os.path.join(data_dir, label)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "\n",
    "        for txt_file in os.listdir(label_path):\n",
    "            txt_path = os.path.join(label_path, txt_file)\n",
    "            if not os.path.isfile(txt_path):\n",
    "                continue\n",
    "\n",
    "            with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                texts.append(f.read())\n",
    "                labels.append(label_id)\n",
    "\n",
    "    return texts, labels, label_map\n",
    "\n",
    "\n",
    "\n",
    "train_texts, train_labels, label_map = load_data_from_folder('dataset/train')\n",
    "test_texts, test_labels, _ = load_data_from_folder('dataset/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d255b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'biznes': 0, 'katastrofa': 1, 'przestępstwo': 2}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5aec9d249d441fca8b3d6d0f75002e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2788 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a788e6966d44b2fb4e9fb45407713fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1190 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(label_map)\n",
    "\n",
    "sbert = SentenceTransformer(\n",
    "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_train = sbert.encode(\n",
    "        train_texts,\n",
    "        convert_to_tensor=True,\n",
    "        batch_size=32,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    X_test = sbert.encode(\n",
    "        test_texts,\n",
    "        convert_to_tensor=True,\n",
    "        batch_size=32,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6202ce1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 1.0947\n",
      "Epoch 2 | Loss: 1.0848\n",
      "Epoch 3 | Loss: 1.0749\n",
      "Epoch 4 | Loss: 1.0646\n",
      "Epoch 5 | Loss: 1.0538\n",
      "Epoch 6 | Loss: 1.0423\n",
      "Epoch 7 | Loss: 1.0298\n",
      "Epoch 8 | Loss: 1.0162\n",
      "Epoch 9 | Loss: 1.0015\n",
      "Epoch 10 | Loss: 0.9855\n",
      "Epoch 11 | Loss: 0.9682\n",
      "Epoch 12 | Loss: 0.9495\n",
      "Epoch 13 | Loss: 0.9292\n",
      "Epoch 14 | Loss: 0.9076\n",
      "Epoch 15 | Loss: 0.8845\n",
      "Epoch 16 | Loss: 0.8603\n",
      "Epoch 17 | Loss: 0.8347\n",
      "Epoch 18 | Loss: 0.8083\n",
      "Epoch 19 | Loss: 0.7805\n",
      "Epoch 20 | Loss: 0.7520\n",
      "Epoch 21 | Loss: 0.7232\n",
      "Epoch 22 | Loss: 0.6940\n",
      "Epoch 23 | Loss: 0.6650\n",
      "Epoch 24 | Loss: 0.6364\n",
      "Epoch 25 | Loss: 0.6079\n",
      "Epoch 26 | Loss: 0.5805\n",
      "Epoch 27 | Loss: 0.5542\n",
      "Epoch 28 | Loss: 0.5290\n",
      "Epoch 29 | Loss: 0.5060\n",
      "Epoch 30 | Loss: 0.4848\n",
      "Epoch 31 | Loss: 0.4651\n",
      "Epoch 32 | Loss: 0.4480\n",
      "Epoch 33 | Loss: 0.4327\n",
      "Epoch 34 | Loss: 0.4192\n",
      "Epoch 35 | Loss: 0.4078\n",
      "Epoch 36 | Loss: 0.3978\n",
      "Epoch 37 | Loss: 0.3895\n",
      "Epoch 38 | Loss: 0.3826\n",
      "Epoch 39 | Loss: 0.3768\n",
      "Epoch 40 | Loss: 0.3723\n",
      "Epoch 41 | Loss: 0.3681\n",
      "Epoch 42 | Loss: 0.3644\n",
      "Epoch 43 | Loss: 0.3614\n",
      "Epoch 44 | Loss: 0.3589\n",
      "Epoch 45 | Loss: 0.3565\n",
      "Epoch 46 | Loss: 0.3545\n",
      "Epoch 47 | Loss: 0.3525\n",
      "Epoch 48 | Loss: 0.3504\n",
      "Epoch 49 | Loss: 0.3488\n",
      "Epoch 50 | Loss: 0.3466\n",
      "Epoch 51 | Loss: 0.3450\n",
      "Epoch 52 | Loss: 0.3433\n",
      "Epoch 53 | Loss: 0.3415\n",
      "Epoch 54 | Loss: 0.3397\n",
      "Epoch 55 | Loss: 0.3375\n",
      "Epoch 56 | Loss: 0.3363\n",
      "Epoch 57 | Loss: 0.3352\n",
      "Epoch 58 | Loss: 0.3338\n",
      "Epoch 59 | Loss: 0.3319\n",
      "Epoch 60 | Loss: 0.3299\n",
      "Epoch 61 | Loss: 0.3287\n",
      "Epoch 62 | Loss: 0.3274\n",
      "Epoch 63 | Loss: 0.3259\n",
      "Epoch 64 | Loss: 0.3250\n",
      "Epoch 65 | Loss: 0.3231\n",
      "Epoch 66 | Loss: 0.3222\n",
      "Epoch 67 | Loss: 0.3216\n",
      "Epoch 68 | Loss: 0.3201\n",
      "Epoch 69 | Loss: 0.3188\n",
      "Epoch 70 | Loss: 0.3183\n",
      "Epoch 71 | Loss: 0.3166\n",
      "Epoch 72 | Loss: 0.3161\n",
      "Epoch 73 | Loss: 0.3151\n",
      "Epoch 74 | Loss: 0.3140\n",
      "Epoch 75 | Loss: 0.3133\n",
      "Epoch 76 | Loss: 0.3124\n",
      "Epoch 77 | Loss: 0.3116\n",
      "Epoch 78 | Loss: 0.3106\n",
      "Epoch 79 | Loss: 0.3101\n",
      "Epoch 80 | Loss: 0.3096\n",
      "Epoch 81 | Loss: 0.3082\n",
      "Epoch 82 | Loss: 0.3079\n",
      "Epoch 83 | Loss: 0.3071\n",
      "Epoch 84 | Loss: 0.3065\n",
      "Epoch 85 | Loss: 0.3059\n",
      "Epoch 86 | Loss: 0.3052\n",
      "Epoch 87 | Loss: 0.3042\n",
      "Epoch 88 | Loss: 0.3033\n",
      "Epoch 89 | Loss: 0.3030\n",
      "Epoch 90 | Loss: 0.3029\n",
      "Epoch 91 | Loss: 0.3023\n",
      "Epoch 92 | Loss: 0.3003\n",
      "Epoch 93 | Loss: 0.3001\n",
      "Epoch 94 | Loss: 0.3000\n",
      "Epoch 95 | Loss: 0.2990\n",
      "Epoch 96 | Loss: 0.2979\n",
      "Epoch 97 | Loss: 0.2982\n",
      "Epoch 98 | Loss: 0.2970\n",
      "Epoch 99 | Loss: 0.2967\n",
      "Epoch 100 | Loss: 0.2963\n",
      "Epoch 101 | Loss: 0.2956\n",
      "Epoch 102 | Loss: 0.2948\n",
      "Epoch 103 | Loss: 0.2944\n",
      "Epoch 104 | Loss: 0.2941\n",
      "Epoch 105 | Loss: 0.2936\n",
      "Epoch 106 | Loss: 0.2930\n",
      "Epoch 107 | Loss: 0.2919\n",
      "Epoch 108 | Loss: 0.2915\n",
      "Epoch 109 | Loss: 0.2912\n",
      "Epoch 110 | Loss: 0.2903\n",
      "Epoch 111 | Loss: 0.2899\n",
      "Epoch 112 | Loss: 0.2894\n",
      "Epoch 113 | Loss: 0.2888\n",
      "Epoch 114 | Loss: 0.2882\n",
      "Epoch 115 | Loss: 0.2877\n",
      "Epoch 116 | Loss: 0.2874\n",
      "Epoch 117 | Loss: 0.2872\n",
      "Epoch 118 | Loss: 0.2868\n",
      "Epoch 119 | Loss: 0.2858\n",
      "Epoch 120 | Loss: 0.2852\n",
      "Epoch 121 | Loss: 0.2841\n",
      "Epoch 122 | Loss: 0.2840\n",
      "Epoch 123 | Loss: 0.2833\n",
      "Epoch 124 | Loss: 0.2837\n",
      "Epoch 125 | Loss: 0.2827\n",
      "Epoch 126 | Loss: 0.2821\n",
      "Epoch 127 | Loss: 0.2814\n",
      "Epoch 128 | Loss: 0.2805\n",
      "Epoch 129 | Loss: 0.2805\n",
      "Epoch 130 | Loss: 0.2797\n",
      "Epoch 131 | Loss: 0.2795\n",
      "Epoch 132 | Loss: 0.2790\n",
      "Epoch 133 | Loss: 0.2781\n",
      "Epoch 134 | Loss: 0.2782\n",
      "Epoch 135 | Loss: 0.2775\n",
      "Epoch 136 | Loss: 0.2768\n",
      "Epoch 137 | Loss: 0.2760\n",
      "Epoch 138 | Loss: 0.2760\n",
      "Epoch 139 | Loss: 0.2749\n",
      "Epoch 140 | Loss: 0.2751\n",
      "Epoch 141 | Loss: 0.2743\n",
      "Epoch 142 | Loss: 0.2737\n",
      "Epoch 143 | Loss: 0.2735\n",
      "Epoch 144 | Loss: 0.2730\n",
      "Epoch 145 | Loss: 0.2726\n",
      "Epoch 146 | Loss: 0.2719\n",
      "Epoch 147 | Loss: 0.2718\n",
      "Epoch 148 | Loss: 0.2710\n",
      "Epoch 149 | Loss: 0.2706\n",
      "Epoch 150 | Loss: 0.2702\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.86      0.88     11074\n",
      "           1       0.89      0.91      0.90     17361\n",
      "           2       0.93      0.90      0.92      9622\n",
      "\n",
      "    accuracy                           0.90     38057\n",
      "   macro avg       0.90      0.89      0.90     38057\n",
      "weighted avg       0.90      0.90      0.90     38057\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 9575  1365   134]\n",
      " [  978 15829   554]\n",
      " [  233   685  8704]]\n"
     ]
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "y_train = torch.tensor(train_labels, dtype=torch.long)\n",
    "y_test  = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "X_train = X_train.clone().detach()\n",
    "X_test  = X_test.clone().detach()\n",
    "\n",
    "X_train = X_train.to(device)\n",
    "X_test  = X_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test  = y_test.to(device)\n",
    "\n",
    "model = Classifier(\n",
    "    input_dim=X_train.shape[1],\n",
    "    num_classes=len(label_map)\n",
    ").to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "    \n",
    "for epoch in range(150):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "THRESHOLD = 0.75\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test)\n",
    "    probs = torch.softmax(logits, dim=1)\n",
    "    max_probs, preds = torch.max(probs, dim=1)\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(test_labels, preds))\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(test_labels, preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6314b7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"pjn_model.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
