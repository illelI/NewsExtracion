{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a60e9126-89e5-45b2-be1a-f402502ffe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from lxml import html\n",
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import spacy\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from sentence_transformers import models, SentenceTransformer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43545dc5-f228-4ff7-b94a-c72c12ec7922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_from_article(url, xpath):\n",
    "    response = urllib.request.urlopen(url)\n",
    "    page_content = response.read()\n",
    "    response.close()\n",
    "    tree = html.fromstring(page_content)\n",
    "    text = tree.xpath(xpath)\n",
    "    return \" \".join(text).strip()\n",
    "\n",
    "def get_wp_articles_urls(url):\n",
    "    response = urllib.request.urlopen(url)\n",
    "    page_content = response.read()\n",
    "    response.close()\n",
    "    tree = html.fromstring(page_content)\n",
    "    xpath = \"//div[@class='akWTRfRe a3brEpa-']//a\"\n",
    "    links = tree.xpath(xpath)\n",
    "    counter = 0\n",
    "    urls = []\n",
    "    for link in links:\n",
    "        if counter % 2 == 0:\n",
    "            urls.append(link.get('href'))\n",
    "        counter+=1\n",
    "    return urls\n",
    "            \n",
    "def create_files(urls, xpath, folder_name):\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "    counter = 1\n",
    "    for url in urls:\n",
    "        text = get_text_from_article(url, xpath)\n",
    "        file_name = os.path.join(folder_name,str(counter) + '.txt')\n",
    "        counter+=1\n",
    "        with open(file_name, 'w', encoding='utf-8') as file:\n",
    "            file.write(text)\n",
    "\n",
    "def get_text_from_wp(url, xpath, folder_name):\n",
    "    try:\n",
    "        urls = get_wp_articles_urls(url)\n",
    "        page = 2\n",
    "        while True:\n",
    "            new_url = url + '/' + str(page)\n",
    "            page+=1\n",
    "            for url1 in get_wp_articles_urls(new_url):\n",
    "                urls.append(url1)\n",
    "    except Exception as e:\n",
    "        create_files(urls, xpath, folder_name)\n",
    "\n",
    "def get_bi_articles_urls(url):\n",
    "    response = urllib.request.urlopen(url)\n",
    "    page_content = response.read()\n",
    "    response.close()\n",
    "    tree = html.fromstring(page_content)\n",
    "    xpath = \"//main//div[@class='stream-list']//a\"\n",
    "    links = tree.xpath(xpath)\n",
    "    urls = []\n",
    "    for link in links:\n",
    "        urls.append(link.get(\"href\"))\n",
    "    return urls\n",
    "\n",
    "def get_text_from_bi(url, xpath, folder_name):\n",
    "    old_size = 0\n",
    "    urls = get_bi_articles_urls(url)\n",
    "    new_size = len(urls)\n",
    "    page = 2\n",
    "    while old_size != new_size:\n",
    "        new_url = url + '?page=' + str(page)\n",
    "        page+=1\n",
    "        old_size = new_size\n",
    "        for url1 in get_bi_articles_urls(new_url):\n",
    "            urls.append(url1)\n",
    "        new_size = len(urls)\n",
    "    create_files(urls, xpath, folder_name)\n",
    "\n",
    "xpath_wp = '//article//h1//text() | //article//li//text() | //article//p//text() | //article//h2//text()'\n",
    "xpath_bi = \"//p[@class='article_p']//text()\"\n",
    "url_przestepstwa = 'https://wiadomosci.wp.pl/tag/przest%C4%99pstwa'\n",
    "url_katastrofa = 'https://wiadomosci.wp.pl/tag/katastrofa'\n",
    "url_biznes = 'https://businessinsider.com.pl/biznes'\n",
    "get_text_from_wp(url_przestepstwa, xpath_wp, 'przestępstwo')\n",
    "get_text_from_wp(url_katastrofa, xpath_wp, 'katastrofa')\n",
    "get_text_from_bi(url_biznes, xpath_bi, 'biznes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc346f5-73ae-4581-8ffc-d90aa80db5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('pl_core_news_lg')\n",
    "\n",
    "\n",
    "def split_text_into_sentences(text):\n",
    "    doc = nlp(text)\n",
    "    return [sent.text for sent in doc.sents]\n",
    "\n",
    "def create_files(category, ratio):\n",
    "    train_counter = 1\n",
    "    test_counter = 1\n",
    "    for filename in os.listdir(category):\n",
    "        with open(category + '/' + filename, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            sentences = split_text_into_sentences(text)\n",
    "            for sentence in sentences:\n",
    "                if random.randint(0, 9) > ratio:\n",
    "                    path = 'dataset/train/' + category + '/' + str(train_counter) + '.txt'\n",
    "                    train_counter+=1\n",
    "                else:\n",
    "                    path = 'dataset/test/' + category + '/' + str(test_counter) + '.txt'\n",
    "                    test_counter+=1\n",
    "                with open(path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(sentence)\n",
    "                \n",
    "\n",
    "def divide_into_test_and_trening(test_size=0.2):\n",
    "    ratio = test_size * 10\n",
    "    categories = ['biznes', 'katastrofa', 'przestępstwo']\n",
    "    try:\n",
    "        shutil.rmtree('dataset')\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    for category in categories:\n",
    "        os.makedirs('dataset/test/' + category, exist_ok=True)\n",
    "        os.makedirs('dataset/train/' + category, exist_ok=True)\n",
    "        create_files(category, ratio)\n",
    "\n",
    "divide_into_test_and_trening()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c72462d-2c8a-43d6-a86e-759550658b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_data_from_folder(data_dir, label_map=None):\n",
    "    texts = []\n",
    "    labels = []\n",
    "\n",
    "    if label_map is None:\n",
    "        label_map = {\n",
    "            d: idx\n",
    "            for idx, d in enumerate(\n",
    "                sorted([\n",
    "                    x for x in os.listdir(data_dir)\n",
    "                    if os.path.isdir(os.path.join(data_dir, x))\n",
    "                ])\n",
    "            )\n",
    "        }\n",
    "\n",
    "    for label, label_id in label_map.items():\n",
    "        label_path = os.path.join(data_dir, label)\n",
    "        if not os.path.isdir(label_path):\n",
    "            continue\n",
    "\n",
    "        for txt_file in os.listdir(label_path):\n",
    "            txt_path = os.path.join(label_path, txt_file)\n",
    "            if not os.path.isfile(txt_path):\n",
    "                continue\n",
    "\n",
    "            with open(txt_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                texts.append(f.read())\n",
    "                labels.append(label_id)\n",
    "\n",
    "    return texts, labels, label_map\n",
    "\n",
    "\n",
    "\n",
    "train_texts, train_labels, label_map = load_data_from_folder('dataset/train')\n",
    "test_texts, test_labels, _ = load_data_from_folder('dataset/test')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9d255b4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "755cfc1b68ee42a691e5ecfa236e2277",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/2080 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "sbert = SentenceTransformer(\n",
    "    \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    device=device\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    X_train = sbert.encode(\n",
    "        train_texts,\n",
    "        convert_to_tensor=True,\n",
    "        batch_size=32,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    X_test = sbert.encode(\n",
    "        test_texts,\n",
    "        convert_to_tensor=True,\n",
    "        batch_size=32,\n",
    "        show_progress_bar=True\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6202ce1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 1.0790\n",
      "Epoch 2 | Loss: 1.0613\n",
      "Epoch 3 | Loss: 1.0437\n",
      "Epoch 4 | Loss: 1.0260\n",
      "Epoch 5 | Loss: 1.0079\n",
      "Epoch 6 | Loss: 0.9888\n",
      "Epoch 7 | Loss: 0.9688\n",
      "Epoch 8 | Loss: 0.9474\n",
      "Epoch 9 | Loss: 0.9246\n",
      "Epoch 10 | Loss: 0.9001\n",
      "Epoch 11 | Loss: 0.8743\n",
      "Epoch 12 | Loss: 0.8469\n",
      "Epoch 13 | Loss: 0.8181\n",
      "Epoch 14 | Loss: 0.7881\n",
      "Epoch 15 | Loss: 0.7575\n",
      "Epoch 16 | Loss: 0.7260\n",
      "Epoch 17 | Loss: 0.6946\n",
      "Epoch 18 | Loss: 0.6633\n",
      "Epoch 19 | Loss: 0.6329\n",
      "Epoch 20 | Loss: 0.6031\n",
      "Epoch 21 | Loss: 0.5746\n",
      "Epoch 22 | Loss: 0.5475\n",
      "Epoch 23 | Loss: 0.5217\n",
      "Epoch 24 | Loss: 0.4972\n",
      "Epoch 25 | Loss: 0.4739\n",
      "Epoch 26 | Loss: 0.4525\n",
      "Epoch 27 | Loss: 0.4321\n",
      "Epoch 28 | Loss: 0.4135\n",
      "Epoch 29 | Loss: 0.3970\n",
      "Epoch 30 | Loss: 0.3822\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.81      0.85     10905\n",
      "           1       0.87      0.94      0.90     17145\n",
      "           2       0.00      0.00      0.00       351\n",
      "\n",
      "    accuracy                           0.88     28401\n",
      "   macro avg       0.59      0.58      0.58     28401\n",
      "weighted avg       0.87      0.88      0.87     28401\n",
      "\n",
      "Confusion matrix:\n",
      "[[ 8879  2026     0]\n",
      " [ 1088 16057     0]\n",
      " [   65   286     0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\Admin\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\sklearn\\metrics\\_classification.py:1731: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "y_train = torch.tensor(train_labels, dtype=torch.long)\n",
    "y_test  = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "X_train = X_train.clone().detach()\n",
    "X_test  = X_test.clone().detach()\n",
    "\n",
    "X_train = X_train.to(device)\n",
    "X_test  = X_test.to(device)\n",
    "y_train = y_train.to(device)\n",
    "y_test  = y_test.to(device)\n",
    "\n",
    "model = Classifier(\n",
    "    input_dim=X_train.shape[1],\n",
    "    num_classes=len(label_map)\n",
    ").to(device)\n",
    "\n",
    "#weights = np.array([0.89, 0.65, 4.0])\n",
    "\n",
    "#weights = torch.tensor(weights, dtype=torch.float).to(device)\n",
    "#criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n",
    "    \n",
    "for epoch in range(30):\n",
    "    model.train()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(X_test)\n",
    "    preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "print(\"\\nClassification report:\")\n",
    "print(classification_report(test_labels, preds))\n",
    "\n",
    "print(\"Confusion matrix:\")\n",
    "print(confusion_matrix(test_labels, preds))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
